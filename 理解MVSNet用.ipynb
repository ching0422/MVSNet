{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a7aa58",
   "metadata": {},
   "source": [
    "# MVSNET 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319306cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "from datasets import find_dataset_def\n",
    "from models import *\n",
    "from utils import *\n",
    "import gc\n",
    "import sys\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef4c80",
   "metadata": {},
   "source": [
    "# load model 預訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64d059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MVSNet(\n",
       "    (feature): FeatureNet(\n",
       "      (conv0): ConvBnReLU(\n",
       "        (conv): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): ConvBnReLU(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): ConvBnReLU(\n",
       "        (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv5): ConvBnReLU(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv6): ConvBnReLU(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (feature): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (cost_regularization): CostRegNet(\n",
       "      (conv0): ConvBnReLU3D(\n",
       "        (conv): Conv3d(32, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): ConvBnReLU3D(\n",
       "        (conv): Conv3d(8, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): ConvBnReLU3D(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): ConvBnReLU3D(\n",
       "        (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): ConvBnReLU3D(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv5): ConvBnReLU3D(\n",
       "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv6): ConvBnReLU3D(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv7): Sequential(\n",
       "        (0): ConvTranspose3d(64, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv9): Sequential(\n",
       "        (0): ConvTranspose3d(32, 16, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv11): Sequential(\n",
       "        (0): ConvTranspose3d(16, 8, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), output_padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (prob): Conv3d(8, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MVSNet(refine=False)\n",
    "model = nn.DataParallel(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load(\"model_000014.ckpt\")\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28531135",
   "metadata": {},
   "source": [
    "# 讀取丟入Model所需之資料 MVS_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the DTU dataset preprocessed by Yao Yao (only for training)\n",
    "\n",
    "# dataset = MVSDataset(\"/home/xyguo/dataset/dtu_mvs/processed/mvs_testing/dtu/\", '../lists/dtu/test.txt', 'test', 5,128)\n",
    "class MVSDataset(Dataset):\n",
    "    def __init__(self, datapath, listfile, mode, nviews, ndepths=192, interval_scale=1.06, **kwargs):\n",
    "        super(MVSDataset, self).__init__()\n",
    "        self.datapath = datapath\n",
    "        self.listfile = listfile\n",
    "        self.mode = mode\n",
    "        self.nviews = nviews\n",
    "        self.ndepths = ndepths\n",
    "        self.interval_scale = interval_scale\n",
    "\n",
    "        assert self.mode == \"test\"\n",
    "        self.metas = self.build_list()\n",
    "\n",
    "    def build_list(self):\n",
    "        metas = []\n",
    "#         with open('/path/to/file', 'r') as f:\n",
    "        with open(self.listfile) as f:\n",
    "            scans = f.readlines()\n",
    "            scans = [line.rstrip() for line in scans]\n",
    "\n",
    "        # scans\n",
    "        for scan in scans:\n",
    "            pair_file = \"{}/pair.txt\".format(scan)\n",
    "            # read the pair file\n",
    "            with open(os.path.join(self.datapath, pair_file)) as f:\n",
    "                num_viewpoint = int(f.readline())\n",
    "                # viewpoints (49)\n",
    "                for view_idx in range(num_viewpoint):\n",
    "                    ref_view = int(f.readline().rstrip())\n",
    "                    src_views = [int(x) for x in f.readline().rstrip().split()[1::2]]\n",
    "                    metas.append((scan, ref_view, src_views))\n",
    "        print(\"dataset\", self.mode, \"metas:\", len(metas))\n",
    "        return metas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metas)\n",
    "\n",
    "    def read_cam_file(self, filename):\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "        # extrinsics: line [1,5), 4x4 matrix\n",
    "        extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ').reshape((4, 4))\n",
    "        # intrinsics: line [7-10), 3x3 matrix\n",
    "        intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ').reshape((3, 3))\n",
    "        intrinsics[:2, :] /= 4\n",
    "        # depth_min & depth_interval: line 11\n",
    "        depth_min = float(lines[11].split()[0])\n",
    "        depth_interval = float(lines[11].split()[1]) * self.interval_scale\n",
    "        return intrinsics, extrinsics, depth_min, depth_interval\n",
    "\n",
    "    def read_img(self, filename):\n",
    "        img = Image.open(filename)\n",
    "        # scale 0~255 to 0~1\n",
    "        np_img = np.array(img, dtype=np.float32) / 255.\n",
    "        assert np_img.shape[:2] == (1200, 1600)\n",
    "        # crop to (1184, 1600)\n",
    "        np_img = np_img[:-16, :]  # do not need to modify intrinsics if cropping the bottom part\n",
    "        return np_img\n",
    "\n",
    "    def read_depth(self, filename):\n",
    "        # read pfm depth file\n",
    "        return np.array(read_pfm(filename)[0], dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        meta = self.metas[idx]\n",
    "        scan, ref_view, src_views = meta\n",
    "        # use only the reference view and first nviews-1 source views\n",
    "        view_ids = [ref_view] + src_views[:self.nviews - 1]\n",
    "\n",
    "        imgs = []\n",
    "        mask = None\n",
    "        depth = None\n",
    "        depth_values = None\n",
    "        proj_matrices = []\n",
    "\n",
    "        for i, vid in enumerate(view_ids):\n",
    "            img_filename = os.path.join(self.datapath, '{}/images/{:0>8}.jpg'.format(scan, vid))\n",
    "            proj_mat_filename = os.path.join(self.datapath, '{}/cams/{:0>8}_cam.txt'.format(scan, vid))\n",
    "\n",
    "            imgs.append(self.read_img(img_filename))\n",
    "            intrinsics, extrinsics, depth_min, depth_interval = self.read_cam_file(proj_mat_filename)\n",
    "\n",
    "            # multiply intrinsics and extrinsics to get projection matrix\n",
    "            proj_mat = extrinsics.copy()\n",
    "            proj_mat[:3, :4] = np.matmul(intrinsics, proj_mat[:3, :4])\n",
    "            proj_matrices.append(proj_mat)\n",
    "\n",
    "            if i == 0:  # reference view\n",
    "                depth_values = np.arange(depth_min, depth_interval * (self.ndepths - 0.5) + depth_min, depth_interval,\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        imgs = np.stack(imgs).transpose([0, 3, 1, 2])\n",
    "        proj_matrices = np.stack(proj_matrices)\n",
    "\n",
    "        return {\"imgs\": imgs,\n",
    "                \"proj_matrices\": proj_matrices,\n",
    "                \"depth_values\": depth_values,\n",
    "                \"filename\": scan + '/{}/' + '{:0>8}'.format(view_ids[0]) + \"{}\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # some testing code, just IGNORE it\n",
    "    dataset = MVSDataset(\"/home/xyguo/dataset/dtu_mvs/processed/mvs_testing/dtu/\", '../lists/dtu/test.txt', 'test', 5,\n",
    "                         128)\n",
    "    item = dataset[50]\n",
    "    for key, value in item.items():\n",
    "        print(key, type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af7d7a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TSHONG~1\\AppData\\Local\\Temp/ipykernel_10408/3836238336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mintrinsics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextrinsics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_cam_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproj_mat_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMVSDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_dataset_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMVSDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtestpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtestlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTestImgLoader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "intrinsics, extrinsics, depth_min, depth_interval = self.read_cam_file(proj_mat_filename)\n",
    "\n",
    "MVSDataset = find_dataset_def(args.dataset)\n",
    "test_dataset = MVSDataset(args.testpath, args.testlist, \"test\", 5, args.numdepth, args.interval_scale)\n",
    "TestImgLoader = DataLoader(test_dataset, args.batch_size, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "# 創建/讀取要餵進去的資料\n",
    "sample = {\"img\":[],\"proj_M\":[],\"depth_val\":[]}\n",
    "path = \"C:/Users/TshongHangY/Desktop/Pytorch_Practice/MVSNet_pytorch-master/DTUDATASET/mvs_training/dtu/\"\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, sample in enumerate(TestImgLoader):\n",
    "#         sample_cuda = tocuda(sample)\n",
    "#         outputs = model(sample_cuda[\"imgs\"], sample_cuda[\"proj_matrices\"], sample_cuda[\"depth_values\"])\n",
    "#         outputs = tensor2numpy(outputs)\n",
    "#         del sample_cuda\n",
    "#         print('Iter {}/{}'.format(batch_idx, len(TestImgLoader)))\n",
    "#         filenames = sample[\"filename\"]\n",
    "\n",
    "#         # save depth maps and confidence maps\n",
    "#         for filename, depth_est, photometric_confidence in zip(filenames, outputs[\"depth\"],\n",
    "#                                                                outputs[\"photometric_confidence\"]):\n",
    "#             depth_filename = os.path.join(args.outdir, filename.format('depth_est', '.pfm'))\n",
    "#             confidence_filename = os.path.join(args.outdir, filename.format('confidence', '.pfm'))\n",
    "#             os.makedirs(depth_filename.rsplit('/', 1)[0], exist_ok=True)\n",
    "#             os.makedirs(confidence_filename.rsplit('/', 1)[0], exist_ok=True)\n",
    "#             # save depth maps\n",
    "#             save_pfm(depth_filename, depth_est)\n",
    "#             # save confidence maps\n",
    "#             save_pfm(confidence_filename, photometric_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7e5d6",
   "metadata": {},
   "source": [
    "# eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import find_dataset_def\n",
    "from models import *\n",
    "from utils import *\n",
    "import sys\n",
    "from datasets.data_io import read_pfm, save_pfm\n",
    "import cv2\n",
    "from plyfile import PlyData, PlyElement\n",
    "from PIL import Image\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Predict depth, filter, and fuse. May be different from the original implementation')\n",
    "parser.add_argument('--model', default='mvsnet', help='select model')\n",
    "\n",
    "parser.add_argument('--dataset', default='dtu_yao_eval', help='select dataset')\n",
    "parser.add_argument('--testpath', help='testing data path')\n",
    "parser.add_argument('--testlist', help='testing scan list')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='testing batch size')\n",
    "parser.add_argument('--numdepth', type=int, default=192, help='the number of depth values')\n",
    "parser.add_argument('--interval_scale', type=float, default=1.06, help='the depth interval scale')\n",
    "\n",
    "parser.add_argument('--loadckpt', default=None, help='load a specific checkpoint')\n",
    "parser.add_argument('--outdir', default='./outputs', help='output dir')\n",
    "parser.add_argument('--display', action='store_true', help='display depth images and masks')\n",
    "\n",
    "# parse arguments and check\n",
    "args = parser.parse_args()\n",
    "print(\"argv:\", sys.argv[1:])\n",
    "print_args(args)\n",
    "\n",
    "\n",
    "# read intrinsics and extrinsics\n",
    "def read_camera_parameters(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.rstrip() for line in lines] # rstrip() 删除 string 字符串末尾的指定字符\n",
    "    # extrinsics: line [1,5), 4x4 matrix\n",
    "    extrinsics = np.fromstring(' '.join(lines[1:5]), dtype=np.float32, sep=' ').reshape((4, 4))\n",
    "    # intrinsics: line [7-10), 3x3 matrix\n",
    "    intrinsics = np.fromstring(' '.join(lines[7:10]), dtype=np.float32, sep=' ').reshape((3, 3))\n",
    "    # TODO: assume the feature is 1/4 of the original image size\n",
    "    intrinsics[:2, :] /= 4\n",
    "    return intrinsics, extrinsics\n",
    "\n",
    "\n",
    "# read an image\n",
    "def read_img(filename):\n",
    "    img = Image.open(filename)\n",
    "    # scale 0~255 to 0~1\n",
    "    np_img = np.array(img, dtype=np.float32) / 255.\n",
    "    return np_img\n",
    "\n",
    "\n",
    "# read a binary mask\n",
    "def read_mask(filename):\n",
    "    return read_img(filename) > 0.5\n",
    "\n",
    "\n",
    "# save a binary mask\n",
    "def save_mask(filename, mask):\n",
    "    assert mask.dtype == np.bool\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "    Image.fromarray(mask).save(filename)\n",
    "\n",
    "\n",
    "# read a pair file, [(ref_view1, [src_view1-1, ...]), (ref_view2, [src_view2-1, ...]), ...]\n",
    "def read_pair_file(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        num_viewpoint = int(f.readline())\n",
    "        # 49 viewpoints\n",
    "        for view_idx in range(num_viewpoint):\n",
    "            ref_view = int(f.readline().rstrip())\n",
    "            src_views = [int(x) for x in f.readline().rstrip().split()[1::2]]\n",
    "            data.append((ref_view, src_views))\n",
    "    return data\n",
    "\n",
    "\n",
    "# run MVS model to save depth maps and confidence maps\n",
    "def save_depth():\n",
    "    # dataset, dataloader\n",
    "    MVSDataset = find_dataset_def(args.dataset)\n",
    "    test_dataset = MVSDataset(args.testpath, args.testlist, \"test\", 5, args.numdepth, args.interval_scale)\n",
    "    TestImgLoader = DataLoader(test_dataset, args.batch_size, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "    # model\n",
    "    model = MVSNet(refine=False)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()\n",
    "\n",
    "    # load checkpoint file specified by args.loadckpt\n",
    "    print(\"loading model {}\".format(args.loadckpt))\n",
    "    state_dict = torch.load(args.loadckpt)\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample in enumerate(TestImgLoader):\n",
    "            sample_cuda = tocuda(sample)\n",
    "            outputs = model(sample_cuda[\"imgs\"], sample_cuda[\"proj_matrices\"], sample_cuda[\"depth_values\"])\n",
    "            outputs = tensor2numpy(outputs)\n",
    "            del sample_cuda\n",
    "            print('Iter {}/{}'.format(batch_idx, len(TestImgLoader)))\n",
    "            filenames = sample[\"filename\"]\n",
    "\n",
    "            # save depth maps and confidence maps\n",
    "            for filename, depth_est, photometric_confidence in zip(filenames, outputs[\"depth\"],\n",
    "                                                                   outputs[\"photometric_confidence\"]):\n",
    "                depth_filename = os.path.join(args.outdir, filename.format('depth_est', '.pfm'))\n",
    "                confidence_filename = os.path.join(args.outdir, filename.format('confidence', '.pfm'))\n",
    "                os.makedirs(depth_filename.rsplit('/', 1)[0], exist_ok=True)\n",
    "                os.makedirs(confidence_filename.rsplit('/', 1)[0], exist_ok=True)\n",
    "                # save depth maps\n",
    "                save_pfm(depth_filename, depth_est)\n",
    "                # save confidence maps\n",
    "                save_pfm(confidence_filename, photometric_confidence)\n",
    "\n",
    "\n",
    "# project the reference point cloud into the source view, then project back\n",
    "def reproject_with_depth(depth_ref, intrinsics_ref, extrinsics_ref, depth_src, intrinsics_src, extrinsics_src):\n",
    "    width, height = depth_ref.shape[1], depth_ref.shape[0]\n",
    "    ## step1. project reference pixels to the source view\n",
    "    # reference view x, y\n",
    "    x_ref, y_ref = np.meshgrid(np.arange(0, width), np.arange(0, height)) # 生成二維座標矩陣\n",
    "    x_ref, y_ref = x_ref.reshape([-1]), y_ref.reshape([-1]) # 壓扁成1維 ??\n",
    "    # reference 3D space # matmul：矩陣相乘 # one_like(X)：全部變1\n",
    "    xyz_ref = np.matmul(np.linalg.inv(intrinsics_ref), \n",
    "                        np.vstack((x_ref, y_ref, np.ones_like(x_ref))) * depth_ref.reshape([-1])) \n",
    "    # source 3D space\n",
    "    xyz_src = np.matmul(np.matmul(extrinsics_src, np.linalg.inv(extrinsics_ref)),\n",
    "                        np.vstack((xyz_ref, np.ones_like(x_ref))))[:3]\n",
    "    # source view x, y\n",
    "    K_xyz_src = np.matmul(intrinsics_src, xyz_src)\n",
    "    xy_src = K_xyz_src[:2] / K_xyz_src[2:3]\n",
    "\n",
    "    ## step2. reproject the source view points with source view depth estimation\n",
    "    # find the depth estimation of the source view\n",
    "    x_src = xy_src[0].reshape([height, width]).astype(np.float32)\n",
    "    y_src = xy_src[1].reshape([height, width]).astype(np.float32)\n",
    "    sampled_depth_src = cv2.remap(depth_src, x_src, y_src, interpolation=cv2.INTER_LINEAR)\n",
    "    # mask = sampled_depth_src > 0\n",
    "\n",
    "    # source 3D space\n",
    "    # NOTE that we should use sampled source-view depth_here to project back\n",
    "    xyz_src = np.matmul(np.linalg.inv(intrinsics_src),\n",
    "                        np.vstack((xy_src, np.ones_like(x_ref))) * sampled_depth_src.reshape([-1]))\n",
    "    # reference 3D space\n",
    "    xyz_reprojected = np.matmul(np.matmul(extrinsics_ref, np.linalg.inv(extrinsics_src)),\n",
    "                                np.vstack((xyz_src, np.ones_like(x_ref))))[:3]\n",
    "    # source view x, y, depth\n",
    "    depth_reprojected = xyz_reprojected[2].reshape([height, width]).astype(np.float32)\n",
    "    K_xyz_reprojected = np.matmul(intrinsics_ref, xyz_reprojected)\n",
    "    xy_reprojected = K_xyz_reprojected[:2] / K_xyz_reprojected[2:3]\n",
    "    x_reprojected = xy_reprojected[0].reshape([height, width]).astype(np.float32)\n",
    "    y_reprojected = xy_reprojected[1].reshape([height, width]).astype(np.float32)\n",
    "\n",
    "    return depth_reprojected, x_reprojected, y_reprojected, x_src, y_src\n",
    "\n",
    "\n",
    "def check_geometric_consistency(depth_ref, intrinsics_ref, extrinsics_ref, depth_src, intrinsics_src, extrinsics_src):\n",
    "    width, height = depth_ref.shape[1], depth_ref.shape[0]\n",
    "    x_ref, y_ref = np.meshgrid(np.arange(0, width), np.arange(0, height))\n",
    "    depth_reprojected, x2d_reprojected, y2d_reprojected, x2d_src, y2d_src = reproject_with_depth(depth_ref, intrinsics_ref, extrinsics_ref,\n",
    "                                                     depth_src, intrinsics_src, extrinsics_src)\n",
    "    # check |p_reproj-p_1| < 1\n",
    "    dist = np.sqrt((x2d_reprojected - x_ref) ** 2 + (y2d_reprojected - y_ref) ** 2)\n",
    "\n",
    "    # check |d_reproj-d_1| / d_1 < 0.01\n",
    "    depth_diff = np.abs(depth_reprojected - depth_ref)\n",
    "    relative_depth_diff = depth_diff / depth_ref\n",
    "\n",
    "    mask = np.logical_and(dist < 1, relative_depth_diff < 0.01)\n",
    "    depth_reprojected[~mask] = 0\n",
    "\n",
    "    return mask, depth_reprojected, x2d_src, y2d_src\n",
    "\n",
    "\n",
    "def filter_depth(scan_folder, out_folder, plyfilename):\n",
    "    # the pair file\n",
    "    pair_file = os.path.join(scan_folder, \"pair.txt\")\n",
    "    # for the final point cloud\n",
    "    vertexs = []\n",
    "    vertex_colors = []\n",
    "\n",
    "    pair_data = read_pair_file(pair_file)\n",
    "    nviews = len(pair_data)\n",
    "    # TODO: hardcode size\n",
    "    # used_mask = [np.zeros([296, 400], dtype=np.bool) for _ in range(nviews)]\n",
    "\n",
    "    # for each reference view and the corresponding source views\n",
    "    for ref_view, src_views in pair_data:\n",
    "        # load the camera parameters\n",
    "        ref_intrinsics, ref_extrinsics = read_camera_parameters(\n",
    "            os.path.join(scan_folder, 'cams/{:0>8}_cam.txt'.format(ref_view)))\n",
    "        # load the reference image\n",
    "        ref_img = read_img(os.path.join(scan_folder, 'images/{:0>8}.jpg'.format(ref_view)))\n",
    "        # load the estimated depth of the reference view\n",
    "        ref_depth_est = read_pfm(os.path.join(out_folder, 'depth_est/{:0>8}.pfm'.format(ref_view)))[0]\n",
    "        # load the photometric mask of the reference view\n",
    "        confidence = read_pfm(os.path.join(out_folder, 'confidence/{:0>8}.pfm'.format(ref_view)))[0]\n",
    "        photo_mask = confidence > 0.8\n",
    "\n",
    "        all_srcview_depth_ests = []\n",
    "        all_srcview_x = []\n",
    "        all_srcview_y = []\n",
    "        all_srcview_geomask = []\n",
    "\n",
    "        # compute the geometric mask\n",
    "        geo_mask_sum = 0\n",
    "        for src_view in src_views:\n",
    "            # camera parameters of the source view\n",
    "            src_intrinsics, src_extrinsics = read_camera_parameters(\n",
    "                os.path.join(scan_folder, 'cams/{:0>8}_cam.txt'.format(src_view)))\n",
    "            # the estimated depth of the source view\n",
    "            src_depth_est = read_pfm(os.path.join(out_folder, 'depth_est/{:0>8}.pfm'.format(src_view)))[0]\n",
    "\n",
    "            geo_mask, depth_reprojected, x2d_src, y2d_src = check_geometric_consistency(ref_depth_est, ref_intrinsics, ref_extrinsics,\n",
    "                                                                      src_depth_est,\n",
    "                                                                      src_intrinsics, src_extrinsics)\n",
    "            geo_mask_sum += geo_mask.astype(np.int32)\n",
    "            all_srcview_depth_ests.append(depth_reprojected)\n",
    "            all_srcview_x.append(x2d_src)\n",
    "            all_srcview_y.append(y2d_src)\n",
    "            all_srcview_geomask.append(geo_mask)\n",
    "\n",
    "        depth_est_averaged = (sum(all_srcview_depth_ests) + ref_depth_est) / (geo_mask_sum + 1)\n",
    "        # at least 3 source views matched\n",
    "        geo_mask = geo_mask_sum >= 3\n",
    "        final_mask = np.logical_and(photo_mask, geo_mask)\n",
    "\n",
    "        os.makedirs(os.path.join(out_folder, \"mask\"), exist_ok=True)\n",
    "        save_mask(os.path.join(out_folder, \"mask/{:0>8}_photo.png\".format(ref_view)), photo_mask)\n",
    "        save_mask(os.path.join(out_folder, \"mask/{:0>8}_geo.png\".format(ref_view)), geo_mask)\n",
    "        save_mask(os.path.join(out_folder, \"mask/{:0>8}_final.png\".format(ref_view)), final_mask)\n",
    "\n",
    "        print(\"processing {}, ref-view{:0>2}, photo/geo/final-mask:{}/{}/{}\".format(scan_folder, ref_view,\n",
    "                                                                                    photo_mask.mean(),\n",
    "                                                                                    geo_mask.mean(), final_mask.mean()))\n",
    "\n",
    "        if args.display:\n",
    "            import cv2\n",
    "            cv2.imshow('ref_img', ref_img[:, :, ::-1])\n",
    "            cv2.imshow('ref_depth', ref_depth_est / 800)\n",
    "            cv2.imshow('ref_depth * photo_mask', ref_depth_est * photo_mask.astype(np.float32) / 800)\n",
    "            cv2.imshow('ref_depth * geo_mask', ref_depth_est * geo_mask.astype(np.float32) / 800)\n",
    "            cv2.imshow('ref_depth * mask', ref_depth_est * final_mask.astype(np.float32) / 800)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        height, width = depth_est_averaged.shape[:2]\n",
    "        x, y = np.meshgrid(np.arange(0, width), np.arange(0, height))\n",
    "        # valid_points = np.logical_and(final_mask, ~used_mask[ref_view])\n",
    "        valid_points = final_mask\n",
    "        print(\"valid_points\", valid_points.mean())\n",
    "        x, y, depth = x[valid_points], y[valid_points], depth_est_averaged[valid_points]\n",
    "        color = ref_img[1:-16:4, 1::4, :][valid_points]  # hardcoded for DTU dataset\n",
    "        xyz_ref = np.matmul(np.linalg.inv(ref_intrinsics),\n",
    "                            np.vstack((x, y, np.ones_like(x))) * depth)\n",
    "        xyz_world = np.matmul(np.linalg.inv(ref_extrinsics),\n",
    "                              np.vstack((xyz_ref, np.ones_like(x))))[:3]\n",
    "        vertexs.append(xyz_world.transpose((1, 0)))\n",
    "        vertex_colors.append((color * 255).astype(np.uint8))\n",
    "\n",
    "        # # set used_mask[ref_view]\n",
    "        # used_mask[ref_view][...] = True\n",
    "        # for idx, src_view in enumerate(src_views):\n",
    "        #     src_mask = np.logical_and(final_mask, all_srcview_geomask[idx])\n",
    "        #     src_y = all_srcview_y[idx].astype(np.int)\n",
    "        #     src_x = all_srcview_x[idx].astype(np.int)\n",
    "        #     used_mask[src_view][src_y[src_mask], src_x[src_mask]] = True\n",
    "\n",
    "    vertexs = np.concatenate(vertexs, axis=0)\n",
    "    vertex_colors = np.concatenate(vertex_colors, axis=0)\n",
    "    vertexs = np.array([tuple(v) for v in vertexs], dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n",
    "    vertex_colors = np.array([tuple(v) for v in vertex_colors], dtype=[('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n",
    "\n",
    "    vertex_all = np.empty(len(vertexs), vertexs.dtype.descr + vertex_colors.dtype.descr)\n",
    "    for prop in vertexs.dtype.names:\n",
    "        vertex_all[prop] = vertexs[prop]\n",
    "    for prop in vertex_colors.dtype.names:\n",
    "        vertex_all[prop] = vertex_colors[prop]\n",
    "\n",
    "    el = PlyElement.describe(vertex_all, 'vertex')\n",
    "    PlyData([el]).write(plyfilename)\n",
    "    print(\"saving the final model to\", plyfilename)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # step1. save all the depth maps and the masks in outputs directory\n",
    "    save_depth()\n",
    "\n",
    "    with open(args.testlist) as f:\n",
    "        scans = f.readlines()\n",
    "        scans = [line.rstrip() for line in scans]\n",
    "\n",
    "    for scan in scans:\n",
    "        scan_id = int(scan[4:])\n",
    "        scan_folder = os.path.join(args.testpath, scan)\n",
    "        out_folder = os.path.join(args.outdir, scan)\n",
    "        # step2. filter saved depth maps with photometric confidence maps and geometric constraints\n",
    "        filter_depth(scan_folder, out_folder, os.path.join(args.outdir, 'mvsnet{:0>3}_l3.ply'.format(scan_id)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
